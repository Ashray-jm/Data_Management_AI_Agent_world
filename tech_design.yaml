DAG_ID: spotify_etl_pipeline
SCHEDULE_INTERVAL: '@daily'
START_DATE: '2023-10-01'
END_DATE: '2023-12-31'
CATCHUP: false
TASKS:
  - task_id: check_for_file_in_s3
    source_file_location: s3://datasets-agentic-ai/spotify_history.csv
    target_table_location:
      database: dev
      schema: spotify_database
      table: spotify_upload_new
    dependencies: []
    sql_query: ''

  - task_id: load_staging_table
    source_file_location: s3://datasets-agentic-ai/spotify_history.csv
    target_table_location:
      database: dev
      schema: spotify_database
      table: spotify_upload_new
    dependencies: [check_for_file_in_s3]
    sql_query: 'COPY spotify_upload_new FROM ''s3://datasets-agentic-ai/spotify_history.csv'' IAM_ROLE ''your_iam_role_here'' CSV;' 

  - task_id: transform_data
    source_file_location: ''
    target_table_location:
      database: dev
      schema: spotify_database
      table: spotify_history_clean
    dependencies: [load_staging_table]
    sql_query: 'CREATE OR REPLACE TABLE spotify_history_clean AS SELECT *, (ms_played::text || platform) as test_column FROM spotify_upload_new;'

METADATA:
  - required: 'Process daily CSV and load into Redshift'
  - execution_time: '06:30 ET for loading and availability by 07:30 ET'
  - acceptance_criteria: 'Identify new CSV, load staging, apply transformations, make available clean table'